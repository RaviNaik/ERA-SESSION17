{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.callbacks import ModelSummary, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.transformer import Transformer\n",
    "from src.data.bert_data import BertDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    initialize(version_base=\"1.3\", config_path=\".\", job_name=\"all\")\n",
    "except ValueError:\n",
    "    hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "    initialize(version_base=\"1.3\", config_path=\".\", job_name=\"all\")\n",
    "\n",
    "cfg = compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = BertDataModule(\n",
    "    seq_len=cfg.bert.seq_len,\n",
    "    n_vocab=cfg.bert.n_vocab,\n",
    "    trainpth=f\"{cfg.bert.data}/training.txt\",\n",
    "    vocabpth=f\"{cfg.bert.data}/vocab.txt\",\n",
    "    batch_size=cfg.bert.batch_size,\n",
    ")\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = Transformer(\n",
    "    arch=\"bert\",\n",
    "    d_model=cfg.bert.embed_size,\n",
    "    max_len=cfg.bert.seq_len,\n",
    "    num_heads=cfg.bert.n_heads,\n",
    "    num_layers=cfg.bert.n_layers,\n",
    "    num_classes=cfg.bert.n_vocab,\n",
    "    d_ff=cfg.bert.inner_ff_size,\n",
    "    dropout=cfg.bert.dropout,\n",
    "    lr=cfg.bert.lr,\n",
    "    weight_decay=cfg.bert.weight_decay,\n",
    "    betas=cfg.bert.betas,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelSummary(max_depth=3),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"./model_checkpoints/bert\",\n",
    "        filename=\"bert_{epoch}\",\n",
    "        monitor=\"train_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    num_nodes=1,\n",
    "    max_epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    limit_val_batches=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /home/ravi.naik/learning/era/s17/s17lit/lightning_logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                             | Type                    | Params\n",
      "------------------------------------------------------------------------------\n",
      "0  | embedding                        | Embeddings              | 5.1 M \n",
      "1  | embedding.lut                    | Embedding               | 5.1 M \n",
      "2  | pe                               | PositionalEncoding      | 0     \n",
      "3  | embedding_dropout                | Dropout                 | 0     \n",
      "4  | transformer_layers               | Sequential              | 1.6 M \n",
      "5  | transformer_layers.0             | TransformerLayer        | 198 K \n",
      "6  | transformer_layers.0.mha         | MultiheadAttention      | 66.0 K\n",
      "7  | transformer_layers.0.ln1         | LayerNorm               | 256   \n",
      "8  | transformer_layers.0.ln2         | LayerNorm               | 256   \n",
      "9  | transformer_layers.0.feedforward | PositionwiseFeedForward | 131 K \n",
      "10 | transformer_layers.1             | TransformerLayer        | 198 K \n",
      "11 | transformer_layers.1.mha         | MultiheadAttention      | 66.0 K\n",
      "12 | transformer_layers.1.ln1         | LayerNorm               | 256   \n",
      "13 | transformer_layers.1.ln2         | LayerNorm               | 256   \n",
      "14 | transformer_layers.1.feedforward | PositionwiseFeedForward | 131 K \n",
      "15 | transformer_layers.2             | TransformerLayer        | 198 K \n",
      "16 | transformer_layers.2.mha         | MultiheadAttention      | 66.0 K\n",
      "17 | transformer_layers.2.ln1         | LayerNorm               | 256   \n",
      "18 | transformer_layers.2.ln2         | LayerNorm               | 256   \n",
      "19 | transformer_layers.2.feedforward | PositionwiseFeedForward | 131 K \n",
      "20 | transformer_layers.3             | TransformerLayer        | 198 K \n",
      "21 | transformer_layers.3.mha         | MultiheadAttention      | 66.0 K\n",
      "22 | transformer_layers.3.ln1         | LayerNorm               | 256   \n",
      "23 | transformer_layers.3.ln2         | LayerNorm               | 256   \n",
      "24 | transformer_layers.3.feedforward | PositionwiseFeedForward | 131 K \n",
      "25 | transformer_layers.4             | TransformerLayer        | 198 K \n",
      "26 | transformer_layers.4.mha         | MultiheadAttention      | 66.0 K\n",
      "27 | transformer_layers.4.ln1         | LayerNorm               | 256   \n",
      "28 | transformer_layers.4.ln2         | LayerNorm               | 256   \n",
      "29 | transformer_layers.4.feedforward | PositionwiseFeedForward | 131 K \n",
      "30 | transformer_layers.5             | TransformerLayer        | 198 K \n",
      "31 | transformer_layers.5.mha         | MultiheadAttention      | 66.0 K\n",
      "32 | transformer_layers.5.ln1         | LayerNorm               | 256   \n",
      "33 | transformer_layers.5.ln2         | LayerNorm               | 256   \n",
      "34 | transformer_layers.5.feedforward | PositionwiseFeedForward | 131 K \n",
      "35 | transformer_layers.6             | TransformerLayer        | 198 K \n",
      "36 | transformer_layers.6.mha         | MultiheadAttention      | 66.0 K\n",
      "37 | transformer_layers.6.ln1         | LayerNorm               | 256   \n",
      "38 | transformer_layers.6.ln2         | LayerNorm               | 256   \n",
      "39 | transformer_layers.6.feedforward | PositionwiseFeedForward | 131 K \n",
      "40 | transformer_layers.7             | TransformerLayer        | 198 K \n",
      "41 | transformer_layers.7.mha         | MultiheadAttention      | 66.0 K\n",
      "42 | transformer_layers.7.ln1         | LayerNorm               | 256   \n",
      "43 | transformer_layers.7.ln2         | LayerNorm               | 256   \n",
      "44 | transformer_layers.7.feedforward | PositionwiseFeedForward | 131 K \n",
      "45 | lm_head                          | Linear                  | 5.2 M \n",
      "46 | softmax                          | Softmax                 | 0     \n",
      "------------------------------------------------------------------------------\n",
      "11.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.465    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6ce9e085fc4856a2c14ff70389ca1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=bert, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
